---
trigger: model_decision
description: When analyzing training results, discussing model performance, or evaluating new experiments
---

# Training Analysis

Loss going down â‰  model learned. Question everything.

## Critical Checklist

### 1. Dataset Quality

| Question | Red Flag |
|----------|----------|
| Class balance? | >70% one class |
| Move diversity? | Repetitive positions |
| Game quality? | Too short (<20) or too long (>200) |
| Dataset size? | <10k samples for serious training |

### 2. Loss Breakdown

**Always analyze separately:**
```
Total = Î±Ã—Policy_Loss + Î²Ã—Value_Loss

Ask:
- Policy loss decreasing? â†’ Learning moves?
- Value loss decreasing? â†’ Learning positions?
- One â†“ other â†‘? â†’ ðŸš© Balance issue Î±/Î²
```

### 3. Real Validation (Not Just Metrics)

| Test | How | What to Check |
|------|-----|---------------|
| **vs baseline** | 100+ games vs vanilla | Win rate >55% significant |
| **vs previous** | 100+ games vs model N-1 | Consistent improvement |
| **Manual inspection** | Watch 5-10 games | Sensible moves? Blunders? |
| **Test positions** | Known positions | Finds right moves? |

## Common Traps

- **"Loss dropped a lot!"** â†’ On what data? Overfitting? Easy dataset?
- **"90% policy accuracy!"** â†’ How many legal moves? (2-3 moves â†’ 90% not impressive)
- **"Value precise!"** â†’ Always predicts ~0? (Safe bet for balanced data)
- **"Improved in tournament!"** â†’ How many games? (<50 = high variance)

## Report Template

```markdown
## Training: [name/date]

**Dataset**:
- Source: [selfplay/tournament/mixed]
- Samples: N
- Distribution: W%/B%/D%

**Metrics**:
- Policy Loss: [start] â†’ [end]
- Value Loss: [start] â†’ [end]
- Epochs: N, LR: X

**Validation**:
- Tournament vs [baseline]: W-L-D (X%)
- Manual observations: [notes]

**Doubts/Limitations**: [list concerns]

**Conclusion**: [only after addressing doubts]
```

> "A model is guilty of not learning until proven innocent."

Never celebrate loss decrease. Celebrate win rate increase.
